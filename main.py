import streamlit as st
import pandas as pd
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

# ==========================================
# 1. ì‚¬ì´íŠ¸ ì„¤ì •
# ==========================================
st.set_page_config(page_title="ê°•ì‚¬ ë°œêµ´ë‹¨", page_icon="ğŸ•µï¸â€â™‚ï¸")
st.title("ğŸ•µï¸â€â™‚ï¸ íƒ€ê²Ÿ ê°•ì‚¬ ìë™ ë°œêµ´ê¸°")
st.write("í¬ëª½ê³¼ í´ë˜ìŠ¤ìœ ì—ì„œ 'AI, ë¶€ì—…, ìë™í™”' ê´€ë ¨ ê°•ì‚¬ë¥¼ ì°¾ìŠµë‹ˆë‹¤.")

# ê²€ìƒ‰í•  í‚¤ì›Œë“œ ì„ íƒí•˜ê¸°
keyword = st.text_input("ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”", value="AI ìë™í™” ìˆ˜ìµ")

# ì‚¬ì´íŠ¸ ì„ íƒí•˜ê¸°
site_option = st.radio("ì–´ë””ë¥¼ ìˆ˜ì§‘í• ê¹Œìš”?", ("í¬ëª½ (Kmong)", "í´ë˜ìŠ¤ìœ  (ClassU)"))

# ==========================================
# 2. í¬ë¡¬ ë¡œë´‡ ì¤€ë¹„ (Headless)
# ==========================================
def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    # ì°¨ë‹¨ ë°©ì§€ë¥¼ ìœ„í•œ ìœ ì € ì—ì´ì „íŠ¸ ì„¤ì •
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    return webdriver.Chrome(
        service=Service(ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()),
        options=chrome_options
    )

# ==========================================
# 3. í¬ëª½ í¬ë¡¤ë§ ë¡œì§
# ==========================================
def crawl_kmong(driver, search_keyword):
    # í¬ëª½ ê²€ìƒ‰ URL
    url = f"https://kmong.com/search?type=gig&keyword={search_keyword}"
    st.info(f"ğŸŒ í¬ëª½ [{search_keyword}] ê²€ìƒ‰ ê²°ê³¼ì— ì ‘ì† ì¤‘...")
    
    driver.get(url)
    time.sleep(3) # ë¡œë”© ëŒ€ê¸°
    
    data_list = []
    
    # í¬ëª½ì€ div íƒœê·¸ êµ¬ì¡°ê°€ ìì£¼ ë°”ë€Œì–´ì„œ ê´‘ë²”ìœ„í•˜ê²Œ ì¡ìŠµë‹ˆë‹¤.
    # ë³´í†µ ìƒí’ˆ ì¹´ë“œë“¤ì´ íŠ¹ì • classë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
    try:
        # ìƒí’ˆ ì¹´ë“œë“¤ ì°¾ê¸° (ê´‘ë²”ìœ„ ì„ íƒì)
        items = driver.find_elements(By.CSS_SELECTOR, 'div[data-testid="search-unit"]')
        
        if len(items) == 0:
            st.warning("í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì„ íƒìê°€ ë³€ê²½ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        for idx, item in enumerate(items[:20]): # ìµœëŒ€ 20ê°œë§Œ ìˆ˜ì§‘
            try:
                # ì œëª© (h3 íƒœê·¸ í˜¹ì€ ë§í¬ ì•ˆì˜ í…ìŠ¤íŠ¸)
                title_elem = item.find_element(By.TAG_NAME, 'h3')
                title = title_elem.text
                
                # ë§í¬
                link_elem = item.find_element(By.TAG_NAME, 'a')
                link = link_elem.get_attribute('href')
                
                # ê°€ê²© (ìˆëŠ” ê²½ìš°)
                try:
                    price = item.find_element(By.CSS_SELECTOR, 'span[class*="price"]').text
                except:
                    price = "ê°€ê²©ë¯¸í‘œê¸°"

                data_list.append({
                    "ì‚¬ì´íŠ¸": "í¬ëª½",
                    "ê°•ì˜/ì„œë¹„ìŠ¤ëª…": title,
                    "ê°€ê²©": price,
                    "ë§í¬": link
                })
            except Exception as e:
                continue # ì—ëŸ¬ë‚˜ë©´ ê±´ë„ˆëœ€
                
    except Exception as e:
        st.error(f"í¬ëª½ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")
        
    return pd.DataFrame(data_list)

# ==========================================
# 4. í´ë˜ìŠ¤ìœ  í¬ë¡¤ë§ ë¡œì§
# ==========================================
def crawl_classu(driver, search_keyword):
    # í´ë˜ìŠ¤ìœ  ê²€ìƒ‰ URL
    url = f"https://www.classu.co.kr/search?keyword={search_keyword}"
    st.info(f"ğŸŒ í´ë˜ìŠ¤ìœ  [{search_keyword}] ê²€ìƒ‰ ê²°ê³¼ì— ì ‘ì† ì¤‘...")
    
    driver.get(url)
    time.sleep(5) # í´ë˜ìŠ¤ìœ ëŠ” ë¡œë”©ì´ ì¢€ ëŠë¦´ ìˆ˜ ìˆìŒ
    
    data_list = []
    
    try:
        # í´ë˜ìŠ¤ìœ  ì¹´ë“œ ì„ íƒì (col-3 ë“± ê·¸ë¦¬ë“œ ì‹œìŠ¤í…œ ì‚¬ìš© ì¶”ì •)
        # 2024ë…„ ê¸°ì¤€ ì¼ë°˜ì ì¸ ì¹´ë“œ í˜•íƒœ ì°¾ê¸°
        items = driver.find_elements(By.CSS_SELECTOR, 'div.col-3') 
        
        if len(items) == 0:
             # ë‹¤ë¥¸ ì„ íƒì ì‹œë„ (êµ¬ì¡° ë³€ê²½ ëŒ€ë¹„)
             items = driver.find_elements(By.CSS_SELECTOR, 'a.c-card')

        for idx, item in enumerate(items[:20]):
            try:
                # ì œëª© ê°€ì ¸ì˜¤ê¸° (div íƒœê·¸ ì¤‘ title í´ë˜ìŠ¤ ë“±)
                text_content = item.text.split('\n')
                
                # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ íŒ¨ìŠ¤
                if len(text_content) < 2:
                    continue
                    
                title = text_content[0] # ë³´í†µ ì²« ì¤„ì´ ì œëª©
                if len(title) < 5: # ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë‘ë²ˆì§¸ ì¤„ì¼ìˆ˜ë„
                    title = text_content[1]
                
                # ë§í¬ ê°€ì ¸ì˜¤ê¸°
                try:
                    link = item.find_element(By.TAG_NAME, 'a').get_attribute('href')
                except:
                    # a íƒœê·¸ ìì²´ê°€ itemì¼ ê²½ìš°
                    link = item.get_attribute('href')
                
                if not link:
                    link = "ë§í¬ ì—†ìŒ"

                data_list.append({
                    "ì‚¬ì´íŠ¸": "í´ë˜ìŠ¤ìœ ",
                    "ê°•ì˜ëª…": title,
                    "ì •ë³´(í…ìŠ¤íŠ¸)": item.text[:50], # ê°•ì‚¬ëª… í¬í•¨ë  ìˆ˜ ìˆìŒ
                    "ë§í¬": link
                })
            except Exception as e:
                continue

    except Exception as e:
        st.error(f"í´ë˜ìŠ¤ìœ  ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")
        
    return pd.DataFrame(data_list)

# ==========================================
# 5. ì‹¤í–‰ ë²„íŠ¼
# ==========================================
if st.button("ê°•ì‚¬ ì°¾ê¸° ì‹œì‘ ğŸš€"):
    driver = get_driver()
    result_df = pd.DataFrame()
    
    with st.spinner('ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...'):
        if "í¬ëª½" in site_option:
            result_df = crawl_kmong(driver, keyword)
        elif "í´ë˜ìŠ¤ìœ " in site_option:
            result_df = crawl_classu(driver, keyword)
    
    driver.quit()
    
    # ê²°ê³¼ ë³´ì—¬ì£¼ê¸°
    if not result_df.empty:
        st.success(f"ì´ {len(result_df)}ê°œì˜ ê°•ì˜ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
        st.dataframe(result_df)
        
        # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
        csv = result_df.to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ“¥ ì—‘ì…€íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
            data=csv,
            file_name=f"{site_option}_{keyword}_ê²°ê³¼.csv",
            mime="text/csv"
        )
    else:
        st.error("ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ê°€ ë„ˆë¬´ êµ¬ì²´ì ì´ê±°ë‚˜ ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë°”ë€Œì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
